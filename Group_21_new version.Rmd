---
title: "Group_21_Analysis"
author: "Group21"
date: "2023-03-13"
output:
  pdf_document:
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadpackages/read data,echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(moderndive)
library(gapminder)
library(kableExtra)
library(gridExtra)
library(skimr)
library(knitr)
library(GGally)
library(tidyverse)
library(sjPlot)
library(stats)
library(jtools)
library(janitor)
library(ggpubr)
library(ResourceSelection)
library(stats)

#Read the data
group21_new <- read.csv("https://raw.githubusercontent.com/ShiruYUan/DAS-Group-21/main/dataset21.csv")
group21_new$sellable_online <- as.factor(group21_new$sellable_online)
group21_new$category <- as.factor(group21_new$category)
group21_new$other_colors <- as.factor(group21_new$other_colors)
```

# Introduction

IKEA furniture is known for its modern and unusual designs, and the price of IKEA furniture is a significant concern for consumers when buying their furniture. The cost of IKEA furniture has a direct impact on consumer trust in the IKEA brand and IKEA's profits. The data in this report comes from official IKEA data, which documents the relationship between the price of different furniture products and other variables. Therefore, our team will study the relationship between the attributes of furniture and furniture with more than 1000 Saudi Riyals.

Section \ref{sec:pre} contains a specific analysis of each variable in the data and discusses how to deal with missing values and investigate the co-linearity between variables. In section \ref{sec:eda}, we summarize the statistical values of the mean, minimum, etc., of each variable in the data and analyze the categorical factors and numerical variables using barplots and boxplots, respectively. Also, the relationship between categorical factors and numerical variables on our dependent variable, furniture price, was analyzed using barplot and boxplot, respectively. In section \ref{sec:fda}, we tested different models using AIC, Hoslem test, etc., until we finally selected the most appropriate model. In section \ref{sec:con}, we summarize the relevant findings from the selected models, and we propose hypotheses and questions for the future work tasks in section \ref{sec:fur}.

## Data description

Our data in this case is about furniture in IKEA Saudi Arabia.

The Features we picked are as below:

-   category -- The furniture category the item belongs to

-   price -- The current price in Saudi Riyals (as recorded on 20/04/2020)

-   sellable_online -- Is the item available to purchase online?

-   other_colors -- Is the item available in other colours

-   depth -- Depth of the item in cm

-   height -- Height of the item in cm

-   width -- width of the item in cm

# Data preprocessing {#sec:pre}

Before analysing the data, we do the data preprocessing. In this part, we choose to use the variables' median to replace its missing value to get our final dataset.

```{r group21_new, warning=FALSE, message=FALSE, include=FALSE}
#Calculates the column median, replacing the missing value with the median
median_depth<- median(group21_new$depth,na.rm = TRUE)
median_depth
group21_new <- group21_new %>%
  mutate(depth = ifelse(is.na(depth), median_depth, depth))

median_height <- median(group21_new$height,na.rm = TRUE)
group21_new <- group21_new %>%
  mutate(height = ifelse(is.na(height), median_height, height))

median_width <- median(group21_new$width,na.rm = TRUE)
median_width
group21_new <- group21_new %>%
  mutate(width = ifelse(is.na(width), median_width, width))

# final dataset
W <- rep(0,500)
group21_new$newprice <- W
group21_new$newprice[group21_new$price>1000]=1
group21_new <- group21_new %>%
  select(category,sellable_online,other_colors,depth,height,width,price,newprice)


group21_new
dim(group21_new)
sum(is.na(group21_new))
```

```{r colinearityNumeric, warning=FALSE, message=FALSE,fig.cap="\\label{cor:numerical}Correlation in numerical variables", fig.width=3, fig.width=5}
#To see if there is colinearity in the numerical variables
ggpairs(group21_new,columns = c(4:6), title = "Correlation between numerical variables",)
```

Figure \ref{cor:numerical} shows that there's a weak relationship between `height` and `depth`(0.019), a mild relationship between `width` and `depth`(0.362) and a moderate relationship between `width` and `height`(0.402)

```{r scaledata, warning=FALSE, message=FALSE}
round(diag(var(group21_new[,4:6])),2) ##significant difference variance
group21_new$depth <- scale(group21_new$depth,center=TRUE, scale=TRUE)
group21_new$height <- scale(group21_new$height,center=TRUE, scale=TRUE)
group21_new$width <- scale(group21_new$width,center=TRUE, scale=TRUE)
```

Based on the result above, we conclude that there is minimal variation between the `depth` of the different products in the data set, with more significant variation in the `height`. Still, the tremendous variation is in the `width` between the products. In other words, the table above means that the `depth` distribution is the smallest, followed by the `height`, and the `width` distribution is the largest. Finally, even the slightest `depth` variation has an enormous value, so we use the scale function below to normalize the data.

Then we use chi-square test to check if there is co-linearity in the three categorical variables

```{r colinearitycategorical, warning=FALSE, message=FALSE,fig.cap="\\label{cor:categorical}Colinearity in categorical variables"}
table_variable_1 <- table(group21_new$category, group21_new$sellable_online)
chisq_result_1 <- chisq.test(table_variable_1)
table_variable_2 <- table(group21_new$category, group21_new$other_colors)
chisq_result__2 <- chisq.test(table_variable_2)
table_variable_3 <- table(group21_new$other_colors, group21_new$sellable_online)
chisq_result_3 <- chisq.test(table_variable_3)
chisq_result_1 #p-value>0.05, there is no colinearity between category and sellable_online.
chisq_result__2#p-value<0.05,there is colinearity between category and other_colors.
chisq_result_3#p-value>0.05, there is no colinearity between other_colors and sellable_online.
```

From the output above, co-llinearity only happens between `category` and `other_colors`, but not between other categorical variables.

```{r, warning=FALSE, message=FALSE,include=FALSE}
group21_new$newprice <- as.factor(group21_new$newprice)
levels(group21_new$newprice) <- c("more", "less")
```

# Explanatory data Analysis {#sec:eda}

### Summary statistics

```{r summary, warning=FALSE, message=FALSE}
#Summary statistic
summary(group21_new)
```

The above summary of the four groups of variables shows that their maximum values differ significantly from the other data, so all four groups have outliers. And the central width portion is spread out furthest between three independent variables.

### Boxplot

A Boxplot is used here to find the relationship between our response variable `newprice` and numeric variables.

```{r boxplot, warning=FALSE, message=FALSE, fig.cap="\\label{box:Newprice} Boxplot of newprice by numeric vairalbes"}
#Boxplot
box1<-ggplot(data = group21_new, aes(x = newprice, y = depth , fill = newprice))+
  geom_boxplot() +
  labs(x = "More or less", y = "Depth")+ 
  theme(legend.position = "none")

box2<-ggplot(data = group21_new, aes(x = newprice, y = height , fill = newprice))+
  geom_boxplot() +
  labs(x = "More or less", y = "Height")+ 
  theme(legend.position = "none")

box3<-ggplot(data = group21_new, aes(x = newprice, y = width , fill = newprice))+
  geom_boxplot() +
  labs(x = "More or less", y = "Width")+ 
  theme(legend.position = "none")
grid.arrange(box1,box2,box3,ncol=3)
```

Figure \ref{box:Newprice} shows three different variables (`depth`, `height`, `width`) in `newprice`. It can be said that there is more difference in Plot 3 between `newprice` and `width`, meaning the `width` will influence the price more than the other two variables.

### Barplot

A barplot is here used to determine the connection between categorical factors and the newprice.

```{r proportion, warning=FALSE, message=FALSE}
#the proportion and barplots(categorical variables)
group21_new %>%
  tabyl(newprice, category) %>%
  adorn_percentages() %>%
  adorn_pct_formatting() %>%
  adorn_ns()

group21_new %>%
  tabyl(sellable_online, newprice) %>%
  adorn_percentages() %>%
  adorn_pct_formatting() %>%
  adorn_ns() # To show original counts

group21_new %>%
  tabyl(other_colors, newprice) %>%
  adorn_percentages() %>%
  adorn_pct_formatting() %>%
  adorn_ns() # To show original counts
```

```{r barplot1, warning=FALSE, message=FALSE, fig.cap="\\label{bar1:Newprice}Boxplot of newprice by categorical vairalbes(category)"}
#Barplot
bar1<-ggplot()+ 
  geom_bar(data =group21_new,
           aes(x = factor(category),fill = factor(newprice)),
           position = "fill")+
  labs(x = "category", y = "newprice")
bar1
```

Figure \ref{bar1:Newprice}, for all categories of furniture, the proportion of most of the "less" is less than 0.5, indicating that the ratio of more than 1000 Saudi Riyals exceeds the balance of less than 1000 Saudi Riyals in almost all furniture, so there is no apparent connection between `category` and our dependent variable `new_price`, and for our analysis, `category` is not a representative variable of whether there is an association between `category` and greater than 1000 Saudi Riyals.

```{r barplot2, warning=FALSE, message=FALSE, fig.cap="\\label{bar2:Newprice}Boxplot of newprice by categorical vairalbes(sellable_online)"}
bar2<-ggplot()+ 
  geom_bar(data=group21_new,
           aes(x=factor(sellable_online),fill = factor(newprice)),
           position="fill")+
  labs(x = "sellable_online", y = "newprice")
bar2
```

Figure \ref{bar2:Newprice},in the "false" part of the above chart, there are no furniture items larger than 1000 Saudi Riyals, which means that all furniture items not sold online are smaller than 1000 Saudi Riyals. Therefore, using `sellable_online` as the independent variable cannot be entirely explained by the relationship with `new_price`.

```{r barplot3, warning=FALSE, message=FALSE, fig.cap="\\label{bar3:Newprice}Boxplot of newprice by categorical vairalbes(other_colors)"}
bar3<-ggplot()+ 
  geom_bar(data =group21_new,
           aes(x = factor(other_colors),fill = factor(newprice)),
           position = "fill")+
  labs(x = "sellable_online", y = "newprice")

bar3
```

Figure \ref{bar3:Newprice}, finally, the table above shows that `other_colors` have a certain percentage of Saudi Riyals, whether larger than 1000 or not, and the difference between the two groups is insignificant. Therefore, we can use `other_colors` as the central modelling premise.

# Formal data analysis {#sec:fda}

In this part we first use the model selection method to pick the our best fitted model from the full model:

```{r model selection, warning=FALSE, message=FALSE}
#Fit a GLM with all variables
model_1 <- glm(newprice ~ category+sellable_online+depth+height+width, data = group21_new,
               family = binomial(link = "logit"))

#Use stepwise selection to select the most important variables
step_model_1 <- step(model_1,direction = "both")#model selection(AIC value)
```

Based on model selection above, the model `newprice~category+height+width` with smallest AIC=366.61 is the best fitted model.

```{r, warning=FALSE, message=FALSE, include=FALSE}
group21_new$newprice<-as.numeric(group21_new$newprice)-1
```

```{r bestmodel, warning=FALSE, message=FALSE}
#Based on AIC pick the best model 
model_2 <- glm(newprice ~ category+height+width, data = group21_new,
               family = binomial(link = "logit"))

#HLtest
hl_1 <- hoslem.test(group21_new$newprice, fitted(model_2), g=10)
hl_1 #p=2.313e-05<0.05, model_2 is not fitted well.
```

By checking the p-value from HL test, we can know that this model doesn't fit well.

Since `category` and `other_colors` are not independent, so next part we remove the `category` variable from the full model and use AIC to check it again.

```{r removeCategory, warning=FALSE, message=FALSE}
model_3 <- glm(newprice ~ sellable_online+other_colors+depth+height+width,
               data = group21_new, family = binomial(link = "logit"))

step_model_3 <- step(model_3,direction = "both")

model_4 <- glm(newprice ~ sellable_online+depth+height+width, data = group21_new,
               family = binomial(link = "logit"))

#HLtest
hl_2 <- hoslem.test(group21_new$newprice, fitted(model_4), g=10)
hl_2  #p=0.000146<0.05, model_4 is not fitted well.
```

From the smallest AIC(434.4), we can tell that `other_color` won't influence the model, so we could remove it from the model. However, from the p-value of the HL test, this model doesn't fit well as well.

Then we want to increase the complexity to our model.

```{r Widecomplexity,warning=FALSE, message=FALSE}
model_5 <- glm(newprice ~ category+sellable_online+depth+height+width+I(depth^2)+I(depth^3),
               data = group21_new, family = binomial(link = "logit"))


step_model_4<- step(model_5,direction = "both")

model_6 <- glm(newprice ~ category + height + width + I(depth^2), data = group21_new,
               family = binomial(link = "logit"))

#HLtest
hl_3 <- hoslem.test(group21_new$newprice, fitted(model_6), g=10)
hl_3  #p=0.0005213<0.05, model_6 is not fitted well.
```

```{r Heightcomplexity, warning=FALSE, message=FALSE}
model_7 <- glm(newprice ~ category+sellable_online+depth+height+width+I(height^2)
               +I(height^3), data = group21_new, family = binomial(link = "logit"))

step_model_5<- step(model_7,direction = "both")

model_8 <- glm(newprice ~ category + depth + height + width + I(height^3),
               data = group21_new, family = binomial(link = "logit"))

#HLtest
hl_4 <- hoslem.test(group21_new$newprice, fitted(model_8), g=10)
hl_4  #p=0.001637<0.05, model_8 is not fitted well.
```

```{r increaseWide3, warning=FALSE, message=FALSE}
##wide^3
model_9<- glm(newprice ~ category+sellable_online+depth+height+width+I(width^2)+I(width^3),
              data = group21_new, family = binomial(link = "logit"))

step_model_6<- step(model_9,direction = "both")

model_10 <- glm(newprice ~ category + height + width + I(width^2) + I(width^3),
                data = group21_new, family = binomial(link = "logit"))

#HLtest
hl_5 <- hoslem.test(group21_new$newprice, fitted(model_10), g=10)
hl_5  #p=0.336>0.05, model_10 is fitted well.
```

```{r increaseWide4, warning=FALSE, message=FALSE}
#wide^4
model_11<- glm(newprice ~ category+sellable_online+depth+height+width+I(width^2)
               +I(width^3)+I(width^4), data = group21_new, family = binomial(link = "logit"))

step_model_7<- step(model_11,direction = "both")

model_12 <- glm(newprice ~ category + depth + height + width + I(width^2) + I(width^3)
                +I(width^4), data = group21_new, family = binomial(link = "logit"))

#HLtest
hl_6 <- hoslem.test(group21_new$newprice, fitted(model_12), g=10)
hl_6  #p=0.5247>0.05,model_12 is fitted well
```

```{r increaseWide4Depth2, warning=FALSE, message=FALSE}
#wide^4,depth^2
model_13<- glm(newprice ~ category+sellable_online+depth+height+width+I(width^2)
               +I(width^3)+I(width^4)+I(depth^2),
               data = group21_new, family = binomial(link = "logit"))

step_model_8<- step(model_13,direction = "both")

model_14 <- glm(newprice ~ category + height + width+I(width^2)+I(width^3)+I(width^4)
                +I(depth^2), data = group21_new, family = binomial(link = "logit"))

#HLtest
hl_7 <- hoslem.test(group21_new$newprice, fitted(model_14), g=10)
hl_7  #p=0.527>0.05, model_14 is fitted well.
```

## Any thoughts on HL test

Based on our knowledge, the Hosmer-Lemeshow test is used for testing model goodness of fit and the test is used in the chi-square test with g-2 degrees of freedom. By running the Hosmer-Lemeshow test, a large chi-squared value with a p-value less than 0.05 indicates a poor fit and p- a value close to 1 indicates an excellent logistic regression model fit. When we first generalized all the independent variables in the dataset to the dependent variable new_price, we found that the p-value was too small and less than 0.05. So we kept trying to extract a few of the independent variables to model new_price, and finally, all the p-values were less than 0.05 and in a small range. The above steps proves that more than simple power-of-one modelling of the independent variables is needed to build a suitable model. Then we add new variables to `depth`, `width` and `height`, respectively, and we find that increasing the power of `depth` and `height` is still less than 0.05 for modelling the p-value, but increasing the capacity of width keeps the p-value close to 1, which means we can make our model fit better. In summary, increasing the `width` strength is the most critical variable in our modelling based on the dataset.

# Conclusions {#sec:con}

When increasing the complexity of the model, it is most beneficial to increase the complexity of the `width` to improve the model's fit directly. The final model build tells us that the `width` variable is an important variable that affects the dependent variable `new_price` and that the p-value of the model becomes more significant as the exponential value of width is increased.

For IKEA furniture larger than 1000 Saudi Riyals, `category` and `sellable_online` have a lot of bias for analyzing the model. In contrast, `other_colors` can help build the most comprehensive relationship with the dependent variable and make our model more convincing.

# Further extension {#sec:fur}

For the further extension, we could check the GLMs' model assumption , to see if the model really fits well. Also could use other method to detect all the models that seems to fit well from the formal analysis, to decide which one will perform the best from those models.
